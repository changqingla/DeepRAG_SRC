#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜å¹¶å‘å¼‚æ­¥æ–‡æ¡£åˆ†å— HTTP æœåŠ¡

è¿™ä¸ªæ¨¡å—æä¾›äº†ä¸€ä¸ªåŸºäº FastAPI çš„é«˜æ€§èƒ½å¼‚æ­¥ HTTP æ¥å£ï¼Œç”¨äºæ–‡æ¡£åˆ†å—å¤„ç†ã€‚
æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹æ€§ï¼š
1. å¼‚æ­¥å¤„ç† - ä½¿ç”¨ asyncio å’Œ FastAPI å®ç°é«˜å¹¶å‘
2. ç›´æ¥å“åº” - å¿«é€Ÿå¤„ç†å¹¶ç›´æ¥è¿”å›ç»“æœ
3. æ™ºèƒ½å¤„ç† - æ ¹æ®æ–‡ä»¶å¤§å°æ™ºèƒ½é€‰æ‹©å¤„ç†ç­–ç•¥
4. é”™è¯¯å¤„ç† - å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
5. ç›‘æ§æŒ‡æ ‡ - æä¾›æ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡ä¿¡æ¯

Author: DeepRAG Team
License: Apache 2.0
"""

import asyncio
import logging
import os
import sys
import time
import uuid
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from typing import Dict, List, Optional, Any, Union
import json
from datetime import datetime, timedelta
import tempfile
import shutil

# FastAPI ç›¸å…³å¯¼å…¥
from fastapi import FastAPI, File, UploadFile, HTTPException, Form
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import uvicorn

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = Path(__file__).parent.absolute()
project_root = current_dir.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥åˆ†å—å™¨
from chunk.document_chunker import DocumentChunker
from rag.utils import ParserType

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    force=True
)
logger = logging.getLogger(__name__)

# ç¡®ä¿æ‰€æœ‰ç›¸å…³æ¨¡å—çš„æ—¥å¿—çº§åˆ«éƒ½æ˜¯ INFO
logging.getLogger('rag.nlp').setLevel(logging.INFO)
logging.getLogger('rag.app.naive').setLevel(logging.INFO)
logging.getLogger('chunk.document_chunker').setLevel(logging.INFO)

# å…¨å±€é…ç½®
class Config:
    """æœåŠ¡é…ç½®"""
    MAX_WORKERS = 10  # æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
    MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB
    LARGE_FILE_SIZE = 10 * 1024 * 1024  # 10MB - å¤§æ–‡ä»¶é˜ˆå€¼
    SUPPORTED_FORMATS = {'.pdf', '.docx', '.doc', '.txt', '.md', '.html', '.pptx', '.xlsx'}
    MAX_CONCURRENT_TASKS = 50  # æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°
    MAX_BATCH_SIZE = 10  # æ‰¹é‡å¤„ç†æœ€å¤§æ–‡ä»¶æ•°
    TEMP_DIR = "/tmp/deeprag_chunks"

# Pydantic æ¨¡å‹å®šä¹‰
class ChunkRequest(BaseModel):
    """åˆ†å—è¯·æ±‚æ¨¡å‹"""
    parser_type: str = Field(default="auto", description="è§£æå™¨ç±»å‹")
    chunk_token_num: int = Field(default=256, ge=1, le=2048, description="æ¯ä¸ªåˆ†å—çš„æœ€å¤§tokenæ•°")
    delimiter: str = Field(default="\nã€‚ï¼›ï¼ï¼Ÿ", description="æ–‡æœ¬åˆ†å‰²ç¬¦")
    language: str = Field(default="Chinese", description="æ–‡æ¡£è¯­è¨€")
    layout_recognize: str = Field(default="DeepDOC", description="å¸ƒå±€è¯†åˆ«æ–¹æ³•")
    zoomin: int = Field(default=3, ge=1, le=10, description="OCRç¼©æ”¾å› å­")
    from_page: int = Field(default=0, ge=0, description="èµ·å§‹é¡µç ")
    to_page: int = Field(default=100000, ge=1, description="ç»“æŸé¡µç ")

class ChunkResponse(BaseModel):
    """åˆ†å—å“åº”æ¨¡å‹"""
    success: bool = Field(description="å¤„ç†æ˜¯å¦æˆåŠŸ")
    chunks: Optional[List[Dict[str, Any]]] = Field(default=None, description="åˆ†å—ç»“æœ")
    total_chunks: int = Field(default=0, description="æ€»åˆ†å—æ•°")
    processing_time: float = Field(description="å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰")
    file_size: int = Field(description="æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰")
    parser_type: str = Field(description="ä½¿ç”¨çš„è§£æå™¨ç±»å‹")
    error: Optional[str] = Field(default=None, description="é”™è¯¯ä¿¡æ¯")

class BatchChunkResponse(BaseModel):
    """æ‰¹é‡åˆ†å—å“åº”æ¨¡å‹"""
    success: bool = Field(description="æ‰¹é‡å¤„ç†æ˜¯å¦æˆåŠŸ")
    results: List[Dict[str, Any]] = Field(description="æ¯ä¸ªæ–‡ä»¶çš„å¤„ç†ç»“æœ")
    total_files: int = Field(description="æ€»æ–‡ä»¶æ•°")
    successful_files: int = Field(description="æˆåŠŸå¤„ç†çš„æ–‡ä»¶æ•°")
    failed_files: int = Field(description="å¤±è´¥çš„æ–‡ä»¶æ•°")
    total_processing_time: float = Field(description="æ€»å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰")

class ServiceStats(BaseModel):
    """æœåŠ¡ç»Ÿè®¡ä¿¡æ¯"""
    uptime: float = Field(description="æœåŠ¡è¿è¡Œæ—¶é—´ï¼ˆç§’ï¼‰")
    total_requests: int = Field(description="æ€»è¯·æ±‚æ•°")
    successful_requests: int = Field(description="æˆåŠŸè¯·æ±‚æ•°")
    failed_requests: int = Field(description="å¤±è´¥è¯·æ±‚æ•°")
    average_processing_time: float = Field(description="å¹³å‡å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰")
    current_concurrent_tasks: int = Field(description="å½“å‰å¹¶å‘ä»»åŠ¡æ•°")

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="DeepRAG æ–‡æ¡£åˆ†å—æœåŠ¡",
    description="é«˜å¹¶å‘å¼‚æ­¥æ–‡æ¡£åˆ†å— HTTP æ¥å£",
    version="2.0.0"
)

# æ·»åŠ  CORS ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€å˜é‡
executor = ThreadPoolExecutor(max_workers=Config.MAX_WORKERS)
semaphore = asyncio.Semaphore(Config.MAX_CONCURRENT_TASKS)

# ç»Ÿè®¡ä¿¡æ¯
stats = {
    "start_time": time.time(),
    "total_requests": 0,
    "successful_requests": 0,
    "failed_requests": 0,
    "processing_times": [],
    "current_concurrent_tasks": 0
}

class ChunkService:
    """ç®€åŒ–çš„åˆ†å—æœåŠ¡ç±»"""
    
    def __init__(self):
        self.temp_dir = Path(Config.TEMP_DIR)
        self.temp_dir.mkdir(exist_ok=True)
    
    @staticmethod
    def validate_file(file: UploadFile) -> bool:
        """éªŒè¯ä¸Šä¼ æ–‡ä»¶"""
        if not file.filename:
            return False
        
        file_ext = Path(file.filename).suffix.lower()
        return file_ext in Config.SUPPORTED_FORMATS
    
    @staticmethod
    def detect_parser_type(filename: str) -> str:
        """æ ¹æ®æ–‡ä»¶åæ£€æµ‹è§£æå™¨ç±»å‹"""
        ext = Path(filename).suffix.lower()
        
        parser_map = {
            '.pdf': "general",
            '.docx': "general",
            '.doc': "general",
            '.txt': "general",
            '.md': "general",
            '.html': "general",
            '.pptx': ParserType.PRESENTATION,
            '.xlsx': ParserType.TABLE,
        }
        
        return parser_map.get(ext, "general")
    
    def save_temp_file(self, file_content: bytes, filename: str) -> str:
        """ä¿å­˜ä¸´æ—¶æ–‡ä»¶"""
        file_id = str(uuid.uuid4())
        temp_file_path = self.temp_dir / f"{file_id}_{filename}"
        
        with open(temp_file_path, "wb") as f:
            f.write(file_content)
        
        return str(temp_file_path)
    
    def cleanup_temp_file(self, file_path: str):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        try:
            if os.path.exists(file_path):
                os.remove(file_path)
        except Exception as e:
            logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
    
    def _process_document_sync(self, file_path: str, request: ChunkRequest) -> Dict[str, Any]:
        """åŒæ­¥å¤„ç†æ–‡æ¡£ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰"""
        try:
            start_time = time.time()
            
            # ç¡®å®šè§£æå™¨ç±»å‹
            if request.parser_type == "auto":
                parser_type = self.detect_parser_type(file_path)
            else:
                parser_type = request.parser_type
            
            # åˆ›å»ºåˆ†å—å™¨å®ä¾‹
            logger.info(f"ğŸ”§ åˆ›å»ºåˆ†å—å™¨ï¼Œå‚æ•°: parser_type={parser_type}, chunk_token_num={request.chunk_token_num}")
            document_chunker = DocumentChunker(
                parser_type=parser_type,
                chunk_token_num=request.chunk_token_num,
                delimiter=request.delimiter,
                language=request.language,
                layout_recognize=request.layout_recognize,
                zoomin=request.zoomin,
                from_page=request.from_page,
                to_page=request.to_page
            )
            
            # æ‰§è¡Œåˆ†å—
            chunks = document_chunker.chunk_document(file_path=file_path)
            processing_time = time.time() - start_time
            
            logger.info(f"ğŸ”§ åˆ†å—å®Œæˆï¼Œç”Ÿæˆäº† {len(chunks)} ä¸ªåˆ†å—ï¼Œè€—æ—¶ {processing_time:.2f}s")
            
            return {
                "success": True,
                "chunks": chunks,
                "total_chunks": len(chunks),
                "processing_time": processing_time,
                "parser_type": parser_type
            }
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡æ¡£å¤±è´¥ {file_path}: {str(e)}")
            return {
                "success": False,
                "chunks": None,
                "total_chunks": 0,
                "processing_time": time.time() - start_time,
                "parser_type": request.parser_type,
                "error": str(e)
            }
    
    async def process_document(self, file_content: bytes, filename: str, request: ChunkRequest) -> Dict[str, Any]:
        """å¼‚æ­¥å¤„ç†å•ä¸ªæ–‡æ¡£"""
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        stats["total_requests"] += 1
        stats["current_concurrent_tasks"] += 1
        
        async with semaphore:  # æ§åˆ¶å¹¶å‘æ•°
            temp_file_path = None
            try:
                # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
                temp_file_path = self.save_temp_file(file_content, filename)
                
                # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œåˆ†å—å¤„ç†
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(
                    executor,
                    self._process_document_sync,
                    temp_file_path,
                    request
                )
                
                # æ·»åŠ æ–‡ä»¶å¤§å°ä¿¡æ¯
                result["file_size"] = len(file_content)
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                if result["success"]:
                    stats["successful_requests"] += 1
                else:
                    stats["failed_requests"] += 1
                
                stats["processing_times"].append(result["processing_time"])
                
                return result
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if temp_file_path:
                    self.cleanup_temp_file(temp_file_path)
                stats["current_concurrent_tasks"] -= 1
    
    async def process_documents_batch(self, files_data: List[tuple], request: ChunkRequest) -> Dict[str, Any]:
        """å¼‚æ­¥æ‰¹é‡å¤„ç†æ–‡æ¡£"""
        start_time = time.time()
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = []
        for file_content, filename in files_data:
            task = self.process_document(file_content, filename, request)
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        processed_results = []
        successful_count = 0
        failed_count = 0
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append({
                    "filename": files_data[i][1],
                    "success": False,
                    "error": str(result),
                    "chunks": None,
                    "total_chunks": 0,
                    "processing_time": 0,
                    "file_size": len(files_data[i][0])
                })
                failed_count += 1
            else:
                processed_results.append({
                    "filename": files_data[i][1],
                    **result
                })
                if result["success"]:
                    successful_count += 1
                else:
                    failed_count += 1
        
        total_processing_time = time.time() - start_time
        
        return {
            "success": failed_count == 0,
            "results": processed_results,
            "total_files": len(files_data),
            "successful_files": successful_count,
            "failed_files": failed_count,
            "total_processing_time": total_processing_time
        }

# æœåŠ¡å®ä¾‹
chunk_service = ChunkService()

# ==================== HTTP æ¥å£ç«¯ç‚¹ ====================

@app.get("/")
async def root():
    """æ ¹è·¯å¾„ - æœåŠ¡çŠ¶æ€"""
    return {
        "service": "DeepRAG æ–‡æ¡£åˆ†å—æœåŠ¡",
        "version": "2.0.0",
        "status": "running",
        "supported_formats": list(Config.SUPPORTED_FORMATS),
        "max_file_size": f"{Config.MAX_FILE_SIZE / 1024 / 1024:.0f}MB",
        "max_concurrent_tasks": Config.MAX_CONCURRENT_TASKS,
        "api_docs": "/docs"
    }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "uptime": time.time() - stats["start_time"],
        "current_concurrent_tasks": stats["current_concurrent_tasks"],
        "total_requests": stats["total_requests"]
    }

@app.post("/chunk", response_model=ChunkResponse)
async def chunk_document(
    file: UploadFile = File(...),
    parser_type: str = Form("auto"),
    chunk_token_num: int = Form(256),
    delimiter: str = Form("\nã€‚ï¼›ï¼ï¼Ÿ"),
    language: str = Form("Chinese"),
    layout_recognize: str = Form("DeepDOC"),
    zoomin: int = Form(3),
    from_page: int = Form(0),
    to_page: int = Form(100000)
):
    """
    æ–‡æ¡£åˆ†å—æ¥å£ - ç›´æ¥è¿”å›åˆ†å—ç»“æœ
    
    æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: PDF, DOCX, DOC, TXT, MD, HTML, PPTX, XLSX
    """
    # éªŒè¯æ–‡ä»¶
    if not chunk_service.validate_file(file):
        raise HTTPException(
            status_code=400,
            detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ã€‚æ”¯æŒçš„æ ¼å¼: {', '.join(Config.SUPPORTED_FORMATS)}"
        )
    
    # è¯»å–æ–‡ä»¶å†…å®¹
    file_content = await file.read()
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    if len(file_content) > Config.MAX_FILE_SIZE:
        raise HTTPException(
            status_code=413,
            detail=f"æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶ ({Config.MAX_FILE_SIZE / 1024 / 1024:.0f}MB)"
        )
    
    # åˆ›å»ºè¯·æ±‚å¯¹è±¡
    request = ChunkRequest(
        parser_type=parser_type,
        chunk_token_num=chunk_token_num,
        delimiter=delimiter,
        language=language,
        layout_recognize=layout_recognize,
        zoomin=zoomin,
        from_page=from_page,
        to_page=to_page
    )
    
    # å¤„ç†æ–‡æ¡£
    result = await chunk_service.process_document(file_content, file.filename, request)
    
    # è¿”å›ç»“æœ
    if result["success"]:
        return ChunkResponse(
            success=True,
            chunks=result["chunks"],
            total_chunks=result["total_chunks"],
            processing_time=result["processing_time"],
            file_size=result["file_size"],
            parser_type=result["parser_type"]
        )
    else:
        raise HTTPException(
            status_code=500,
            detail=f"æ–‡æ¡£åˆ†å—å¤„ç†å¤±è´¥: {result['error']}"
        )

@app.post("/chunk/batch", response_model=BatchChunkResponse)
async def chunk_documents_batch(
    files: List[UploadFile] = File(...),
    parser_type: str = Form("auto"),
    chunk_token_num: int = Form(256),
    delimiter: str = Form("\nã€‚ï¼›ï¼ï¼Ÿ"),
    language: str = Form("Chinese"),
    layout_recognize: str = Form("DeepDOC"),
    zoomin: int = Form(3),
    from_page: int = Form(0),
    to_page: int = Form(100000)
):
    """
    æ‰¹é‡æ–‡æ¡£åˆ†å—æ¥å£ - åŒæ—¶å¤„ç†å¤šä¸ªæ–‡æ¡£
    
    æœ€å¤šæ”¯æŒåŒæ—¶å¤„ç† 10 ä¸ªæ–‡ä»¶
    """
    # æ£€æŸ¥æ–‡ä»¶æ•°é‡
    if len(files) > Config.MAX_BATCH_SIZE:
        raise HTTPException(
            status_code=400,
            detail=f"æ‰¹é‡ä¸Šä¼ æ–‡ä»¶æ•°é‡ä¸èƒ½è¶…è¿‡ {Config.MAX_BATCH_SIZE} ä¸ª"
        )
    
    # éªŒè¯å’Œè¯»å–æ–‡ä»¶
    files_data = []
    for file in files:
        if not chunk_service.validate_file(file):
            raise HTTPException(
                status_code=400,
                detail=f"æ–‡ä»¶ {file.filename} æ ¼å¼ä¸æ”¯æŒ"
            )
        
        file_content = await file.read()
        
        if len(file_content) > Config.MAX_FILE_SIZE:
            raise HTTPException(
                status_code=413,
                detail=f"æ–‡ä»¶ {file.filename} å¤§å°è¶…è¿‡é™åˆ¶"
            )
        
        files_data.append((file_content, file.filename))
    
    # åˆ›å»ºè¯·æ±‚å¯¹è±¡
    request = ChunkRequest(
        parser_type=parser_type,
        chunk_token_num=chunk_token_num,
        delimiter=delimiter,
        language=language,
        layout_recognize=layout_recognize,
        zoomin=zoomin,
        from_page=from_page,
        to_page=to_page
    )
    
    # æ‰¹é‡å¤„ç†æ–‡æ¡£
    result = await chunk_service.process_documents_batch(files_data, request)
    
    return BatchChunkResponse(**result)

@app.get("/stats", response_model=ServiceStats)
async def get_statistics():
    """
    è·å–æœåŠ¡ç»Ÿè®¡ä¿¡æ¯
    """
    uptime = time.time() - stats["start_time"]
    avg_processing_time = (
        sum(stats["processing_times"]) / len(stats["processing_times"])
        if stats["processing_times"] else 0
    )
    
    return ServiceStats(
        uptime=uptime,
        total_requests=stats["total_requests"],
        successful_requests=stats["successful_requests"],
        failed_requests=stats["failed_requests"],
        average_processing_time=avg_processing_time,
        current_concurrent_tasks=stats["current_concurrent_tasks"]
    )

@app.post("/admin/cleanup")
async def cleanup_temp_files():
    """
    æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    
    åˆ é™¤ä¸´æ—¶ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
    """
    try:
        temp_dir = Path(Config.TEMP_DIR)
        if temp_dir.exists():
            file_count = len(list(temp_dir.glob("*")))
            shutil.rmtree(temp_dir, ignore_errors=True)
            temp_dir.mkdir(exist_ok=True)
            
            return {
                "message": f"å·²æ¸…ç† {file_count} ä¸ªä¸´æ—¶æ–‡ä»¶",
                "cleaned_files": file_count
            }
        else:
            return {
                "message": "ä¸´æ—¶ç›®å½•ä¸å­˜åœ¨",
                "cleaned_files": 0
            }
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}"
        )

@app.on_event("startup")
async def startup_event():
    """æœåŠ¡å¯åŠ¨äº‹ä»¶"""
    logger.info("DeepRAG æ–‡æ¡£åˆ†å—æœåŠ¡å¯åŠ¨ä¸­...")
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    temp_dir = Path(Config.TEMP_DIR)
    temp_dir.mkdir(exist_ok=True)
    
    logger.info(f"æœåŠ¡é…ç½®:")
    logger.info(f"  - æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°: {Config.MAX_WORKERS}")
    logger.info(f"  - æœ€å¤§æ–‡ä»¶å¤§å°: {Config.MAX_FILE_SIZE / 1024 / 1024:.0f}MB")
    logger.info(f"  - æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°: {Config.MAX_CONCURRENT_TASKS}")
    logger.info(f"  - æœ€å¤§æ‰¹é‡å¤„ç†æ•°: {Config.MAX_BATCH_SIZE}")
    logger.info(f"  - æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {', '.join(Config.SUPPORTED_FORMATS)}")
    logger.info(f"  - ä¸´æ—¶æ–‡ä»¶ç›®å½•: {Config.TEMP_DIR}")
    logger.info("DeepRAG æ–‡æ¡£åˆ†å—æœåŠ¡å¯åŠ¨å®Œæˆ!")

@app.on_event("shutdown")
async def shutdown_event():
    """æœåŠ¡å…³é—­äº‹ä»¶"""
    logger.info("DeepRAG æ–‡æ¡£åˆ†å—æœåŠ¡å…³é—­ä¸­...")
    
    # å…³é—­çº¿ç¨‹æ± 
    executor.shutdown(wait=True)
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    temp_dir = Path(Config.TEMP_DIR)
    if temp_dir.exists():
        shutil.rmtree(temp_dir, ignore_errors=True)
    
    logger.info("DeepRAG æ–‡æ¡£åˆ†å—æœåŠ¡å·²å…³é—­")

# ==================== æœåŠ¡å¯åŠ¨ ====================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="DeepRAG æ–‡æ¡£åˆ†å—æœåŠ¡")
    parser.add_argument("--host", default="0.0.0.0", help="æœåŠ¡ä¸»æœºåœ°å€")
    parser.add_argument("--port", type=int, default=8089, help="æœåŠ¡ç«¯å£")
    parser.add_argument("--workers", type=int, default=1, help="å·¥ä½œè¿›ç¨‹æ•°")
    parser.add_argument("--reload", action="store_true", help="å¼€å‘æ¨¡å¼è‡ªåŠ¨é‡è½½")
    parser.add_argument("--log-level", default="info", help="æ—¥å¿—çº§åˆ«")
    
    args = parser.parse_args()
    
    logger.info(f"å¯åŠ¨ DeepRAG æ–‡æ¡£åˆ†å—æœåŠ¡...")
    logger.info(f"æœåŠ¡åœ°å€: http://{args.host}:{args.port}")
    logger.info(f"API æ–‡æ¡£: http://{args.host}:{args.port}/docs")
    
    uvicorn.run(
        "chunk_server:app",
        host=args.host,
        port=args.port,
        workers=args.workers,
        reload=args.reload,
        log_level=args.log_level,
        access_log=True
    )
